# Proposal 008: Resolve Topic Names from Topic IDs in the Filter Framework

## Summary

This proposal outlines a new feature for the Kroxylicious filter framework: a mechanism to resolve topic names from their corresponding topic IDs. This will enable filters to implement business logic based on human-readable topic names, rather than the underlying Kafka implementation detail of topic IDs.

---

## Current Situation

Currently, the Kroxylicious framework does not provide a direct way for filters to translate a topic ID back into its topic name. Filters that receive requests containing only topic IDs have no simple, framework-supported method to determine the names of the topics they are interacting with.

---

## Motivation

Apache Kafka increasingly uses **Topic IDs** as unique and stable identifiers for topics, especially in newer RPCs. This approach robustly handles edge cases, such as when a topic is deleted and later recreated with the same name. You can find more details in [KIP-516: Topic Identifiers](https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers). In Kafka 4.1.0 Produce Requests will begin using
topic ids, impacting multiple Filters offered by the project.

However, humans and business rules operate on **topic names**, not opaque IDs. For example, a policy is typically defined as "Encrypt data for the `sensitive_data` topic" or "Validate schemas for the `invoices` topic." Kroxylicious filters, which enforce these policies, are naturally designed to work with topic names.

To bridge this gap and allow filters to operate effectively, the framework must provide a reliable way to convert topic IDs back to their corresponding names.

---

## Proposal

I propose:
1. adding a new method to the `FilterContext` interface that allows filters to look up topic names for a given set of topic IDs.
2. This will drive a request to the upstream to learn the topic names. 
3. We will cache the topic names at the edge of the proxy/upstream per channel.
4. Existing Filters that mutate MetadataResponse will compose with this API without modification.

### 1. API Addition

The following method will be added to the `FilterContext`:

```java
/**
 * Asynchronously resolves a set of topic UUIDs to their corresponding topic names.
 *
 * @param topicUuids A Set of topic UUIDs to resolve.
 * @return A CompletionStage that will complete with an unmodifiable Map<Uuid, String>,
 * mapping each topic UUID to its topic name. The stage will complete
 * exceptionally if any UUID in the set cannot be resolved.
 */
CompletionStage<Map<Uuid, String>> getTopicNames(Set<Uuid> topicUuids);
```

This method will internally send a `Metadata` request to the upstream Kafka cluster to fetch the required topic information.

The operation will fail atomically to ensure filters do not operate on a partial or incomplete mapping of IDs to names.

### 2. Composability

A core value of Kroxylicious is filter composability. Other filters with virtualization capabilities (e.g., a multitenancy filter) may need to alter the topic names presented to downstream filters.

To support this, the underlying `Metadata` request generated by this new method will flow through the standard filter chain. This allows any filter to intercept and mutate the `MetadataResponse`, ensuring seamless composition with existing filters like `Multitenant` without requiring any changes to them.

### 3. Performance and Caching

To avoid overwhelming the upstream cluster with frequent `Metadata` requests, the responses from this lookup should be cached. Equally we do not wish to impede client and other internal requests, we 
should forward them to the upstream.

I propose marking these framework-generated requests as a special class of "edge-cacheable" requests. This tells the framework that it is safe to learn the topic names and produce short-circuit responses without calling out to the upstream cluster, reducing redundant network traffic. 

The 'marking' can be implemented by tagging the RequestFrame and propagating it through the correlation manager. Then we can
configure a caching filter to only intercept requests and responses with that tag. This would be a framework only feature, not surfaced in the Filter API.

The 'edge' aspect is saying that we want to cache what the upstream said the topic names are, at the edge of the Proxy -> Upstream, but they must then traverse the Filter chain to maintain composability.

We can implement a very long lived cache invalidation strategy as Kafkatopics cannot be renamed. Inconsistencies will arise if the upstream topics are deleted but we should be able to rely on the upstream failing if we forward any interactions with deleted topics.

### 4. Security

Kafka ACLs may restrict a user's access to topic metadata. We must not introduce a security vulnerability that leaks information about topics to unauthorized users.

Therefore, the initial implementation of the cache will be **per-channel**. This approach scopes the cache 1:1 with an upstream connection and its authenticated principal, ensuring that a user can only receive metadata they are authorized to see. In the future, we could explore a virtual-cluster-level cache, but this would require users to explicitly opt-in after being warned of the potential security trade-offs.

---

## Affected Projects

* **kroxylicious-api**: Affected by the addition of the new method to the `FilterContext` interface.
* **kroxylicious-runtime**: Affected by the implementation of the topic name resolution logic, including caching and request handling.

---

## Compatibility

The change to `FilterContext` is **backwards compatible**, as it involves adding a new default method.

The underlying `Metadata` request will use the lowest API version capable of looking up topics by ID. The rationale is that if a client has received topic IDs, the upstream broker must be capable of resolving them via a `Metadata` request.

---

## Rejected Alternatives

An alternative considered was to introduce a new, specialized API for these resource requests instead of using the existing `Metadata` RPC flow. Filters like `Multitenant` would then need to implement this new API to intercept and modify the name lookup.

* **Downside**: This approach complicates development for plugin authors by requiring them to implement an additional API. The existing filter chain is already well-suited for intercepting Kafka RPCs, and the short-circuiting logic for caching is already in place.
* **Upside**: A custom API could potentially be more optimized, as it would avoid the overhead of creating Kafka RPC objects.

Ultimately, reusing the existing, familiar `Metadata` filter mechanism was chosen as it provides better composability and a simpler developer experience, which outweighs the minor potential for optimization.
